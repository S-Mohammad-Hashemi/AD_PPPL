{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-Mohammad-Hashemi/AD_PPPL/blob/main/PPPL_Scenario1_CICIDS2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "2EPrBkcDyP-q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/S-Mohammad-Hashemi/PPPL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPQDD44l2V2f",
        "outputId": "e51df49b-528d-48a5-cf72-cd5ac139e690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PPPL'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 71 (delta 17), reused 5 (delta 2), pack-reused 38\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n",
            "Checking out files: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PPPL/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjlMVZKx2aOI",
        "outputId": "ecd5dc50-3b0f-419f-dfd7-0ef88bd33fc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PPPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./extract_cicids_dataset.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjAv9JCF2fmw",
        "outputId": "1378bb44-d784-49e3-c6c1-d02ad950b9dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  datasets/CICIDS2017_packet-based/tuesday.zip\n",
            "   creating: datasets/CICIDS2017_packet-based/tuesday/\n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/labels.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00000.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00001.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00002.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00003.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00004.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00005.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00006.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00007.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00008.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00009.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00010.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00011.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00012.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00013.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00014.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00015.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00016.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00017.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00018.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00019.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00020.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00021.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00022.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/tuesday/part_00023.npy  \n",
            "Archive:  datasets/CICIDS2017_packet-based/wednesday.zip\n",
            "   creating: datasets/CICIDS2017_packet-based/wednesday/\n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/labels.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00000.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00001.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00002.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00003.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00004.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00005.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00006.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00007.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00008.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00009.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00010.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00011.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00012.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00013.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00014.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00015.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00016.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00017.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00018.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00019.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00020.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00021.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00022.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00023.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00024.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00025.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00026.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00027.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/wednesday/part_00028.npy  \n",
            "Archive:  datasets/CICIDS2017_packet-based/thursday.zip\n",
            "   creating: datasets/CICIDS2017_packet-based/thursday/\n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/labels.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00000.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00001.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00002.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00003.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00004.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00005.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00006.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00007.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00008.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00009.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00010.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00011.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00012.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00013.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00014.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00015.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00016.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00017.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00018.npy  \n",
            "  inflating: datasets/CICIDS2017_packet-based/thursday/part_00019.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset\n",
        "dataset_path='./datasets/CICIDS2017_packet-based/' \n",
        "\n",
        "datasets = {\n",
        "    'A':'tuesday',\n",
        "    'B':'wednesday',\n",
        "    'C':'thursday'\n",
        "    }\n",
        "\n",
        "#### src_domain and trg_domain can be any of the above domains.\n",
        "s_domain = datasets['A'] #source domain\n",
        "t_domain = datasets['B'] #target domain"
      ],
      "metadata": {
        "id": "t0pz-qmX3I_2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "pZNN4eBa25uF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00df8bf-3b80-49f6-ef03-bc5e722bfb74"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import  preproces_dataset, Solver, PacketModel, DataHandler\n"
      ],
      "metadata": {
        "id": "oqVbWFZ51QsG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading and preprocessing data"
      ],
      "metadata": {
        "id": "vhSePmL83-qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_src,y_src,x_trg,y_trg,train_min,train_max = preproces_dataset(s_domain,t_domain,dataset_path)\n",
        "\n",
        "##### Uncomment the next two lines to make the training procedure faster!\n",
        "# x_trg = x_trg[::2]\n",
        "# y_trg = y_trg[::2]\n",
        "\n",
        "y_src_onehot = np.zeros((len(y_src),2),np.float32)\n",
        "y_src_onehot[range(len(y_src)),y_src.astype(np.int32)] = 1.\n",
        "\n",
        "y_trg_onehot = np.zeros((len(y_trg),2),np.float32)\n",
        "y_trg_onehot[range(len(y_trg)),y_trg.astype(np.int32)] = 1.\n",
        "\n",
        "print('x_src.shape:',x_src.shape,'y_src.shape:',y_src.shape,'x_trg.shape:',x_trg.shape,'y_trg.shape:',y_trg.shape)\n",
        "print('Malicious ratio in the source domain:',np.sum(y_src==1)/len(y_src))\n",
        "print('Malicious ratio in the target domain:',np.sum(y_trg==1)/len(y_trg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1TH3O4w4KsP",
        "outputId": "ac7192ac-b4e3-4414-8e7d-229c9670b607"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./datasets/CICIDS2017_packet-based/tuesday/part_00000.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00001.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00002.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00003.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00004.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00005.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00006.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00007.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00008.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00009.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00010.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00011.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00012.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00013.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00014.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00015.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00016.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00017.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00018.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00019.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00020.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00021.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00022.npy\n",
            "./datasets/CICIDS2017_packet-based/tuesday/part_00023.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00000.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00001.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00002.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00003.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00004.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00005.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00006.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00007.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00008.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00009.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00010.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00011.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00012.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00013.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00014.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00015.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00016.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00017.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00018.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00019.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00020.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00021.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00022.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00023.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00024.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00025.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00026.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00027.npy\n",
            "./datasets/CICIDS2017_packet-based/wednesday/part_00028.npy\n",
            "x_src.shape: (573544, 580) y_src.shape: (573544,) x_trg.shape: (685241, 580) y_trg.shape: (685241,)\n",
            "Malicious ratio in the source domain: 0.021738523984210452\n",
            "Malicious ratio in the target domain: 0.18153175306206137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model on the src domain\n"
      ],
      "metadata": {
        "id": "ftk6Vkg74_1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = x_src.shape[1]\n",
        "net = PacketModel()\n",
        "net._set_inputs(tf.TensorSpec([None,input_size]))\n",
        "\n",
        "base_lr = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)\n",
        "grads = []\n",
        "for v in net.trainable_variables:\n",
        "    grads.append(np.zeros(v.shape))\n",
        "optimizer.apply_gradients(zip(grads,net.trainable_variables))\n",
        "\n",
        "tsolver = Solver(optimizer,net,base_lr)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "metadata": {
        "id": "QOo4tjrf5bRQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step_eager(x, y, optimizer,net):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = net(x, training=True)\n",
        "        mse_loss = tf.reduce_sum((y - y_pred)**2,axis=1)\n",
        "        mse_loss = tf.reduce_mean(mse_loss)\n",
        "    gradients = tape.gradient(mse_loss, net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
        "    train_loss(mse_loss)\n"
      ],
      "metadata": {
        "id": "nX583yC95a-H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_on_src(nb_iters,dhandler,tsolver):\n",
        "    st = time.time()\n",
        "    train_loss.reset_states()\n",
        "    for i in range(nb_iters):\n",
        "        x_batch,y_batch = dhandler.next_batch()\n",
        "\n",
        "        if i%5==0:\n",
        "            tsolver.iters+=1\n",
        "            tsolver.update_lr()\n",
        "\n",
        "        train_step(x_batch,y_batch,tsolver.optimizer,tsolver.net)\n",
        "        if i % 50 == 49 or i == nb_iters - 1:\n",
        "            remained_iters = nb_iters - i\n",
        "            passed_time = time.time() - st\n",
        "            ETA = int(passed_time * remained_iters / i)\n",
        "            ETA_min, ETA_sec = ETA // 60, ETA % 60\n",
        "            mean_loss = train_loss.result().numpy()\n",
        "            print ('\\r' + \\\n",
        "                  ' iter: ' + str(i + 1) + '/' + str(nb_iters) + \\\n",
        "                  ' ETA: ' + str(ETA_min) + ':' + \"{0:02d}\".format(ETA_sec) + \\\n",
        "                  ' loss: ' + \"{0:0.4f}\".format(mean_loss),end=\" \")\n",
        "            sys.stdout.flush()\n",
        "    print(' ')\n",
        "\n"
      ],
      "metadata": {
        "id": "0o1fLVa_5axe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:15.083401Z",
          "start_time": "2021-06-15T22:52:15.051590Z"
        },
        "id": "YoOeex1n0-ry"
      },
      "outputs": [],
      "source": [
        "train_step = tf.function(train_step_eager)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:15.429404Z",
          "start_time": "2021-06-15T22:52:15.405785Z"
        },
        "id": "zDNVcT260-rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c6adcf-4239-44c2-b2c7-2ba783e6f2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " iter: 2241/2241 ETA: 0:00 loss: 0.0369  \n",
            " iter: 2241/2241 ETA: 0:00 loss: 0.0151  \n",
            " iter: 2241/2241 ETA: 0:00 loss: 0.0123  \n",
            " iter: 2241/2241 ETA: 0:00 loss: 0.0100  \n",
            " iter: 2241/2241 ETA: 0:00 loss: 0.0085  \n",
            " iter: 2241/2241 ETA: 0:00 loss: 0.0064  \n"
          ]
        }
      ],
      "source": [
        "nb_epochs = 6\n",
        "batch_size = 256\n",
        "total_batch = len(x_src)//batch_size\n",
        "if len(x_src) % batch_size!=0:\n",
        "    total_batch+=1\n",
        "RANDOM_SEED = 2022\n",
        "rng = np.random.RandomState(RANDOM_SEED)\n",
        "\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    pos_inds = y_src==1\n",
        "    x_src_pos = x_src[pos_inds]\n",
        "    y_src_pos = y_src_onehot[pos_inds]\n",
        "    x_src_neg = x_src[~pos_inds]\n",
        "    y_src_neg = y_src_onehot[~pos_inds]\n",
        "    p = np.random.permutation(len(x_src_neg))\n",
        "    x_src_neg = x_src_neg[p]\n",
        "    y_src_neg = y_src_neg[p]\n",
        "    pos_len = len(x_src_pos)\n",
        "\n",
        "    src_dhandler = DataHandler(np.concatenate((x_src_pos,x_src_neg[:pos_len]))\n",
        "                                        ,np.concatenate((y_src_pos,y_src_neg[:pos_len])),None,batch_size=256,shuffle=True)\n",
        "\n",
        "    train_model_on_src(total_batch,src_dhandler,tsolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:15.650672Z",
          "start_time": "2021-06-15T22:52:15.627884Z"
        },
        "id": "zmY8cf9R0-r0"
      },
      "outputs": [],
      "source": [
        "def test_step_eager(x,net):\n",
        "    return net(x,training=False)\n",
        "\n",
        "test_step = tf.function(test_step_eager)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:15.938881Z",
          "start_time": "2021-06-15T22:52:15.914121Z"
        },
        "id": "FLoE3Api0-r1"
      },
      "outputs": [],
      "source": [
        "def test(x_ds,y_ds,net,ret=False):\n",
        "    all_y_pred = np.zeros_like(y_ds)\n",
        "    all_scores = np.zeros((len(y_ds),2))\n",
        "    for i in range(0,len(x_ds),batch_size):\n",
        "        x_batch = x_ds[i:i+batch_size]\n",
        "        y_batch = y_ds[i:i+batch_size]\n",
        "        y_pred = test_step(x_batch,net)\n",
        "        y_pred = y_pred.numpy()\n",
        "        all_scores[i:i+batch_size] = y_pred\n",
        "        y_pred = y_pred.argmax(axis=1)\n",
        "        all_y_pred[i:i+batch_size] = y_pred\n",
        "    if ret:\n",
        "        return all_y_pred,all_scores\n",
        "    print('accuracy:',np.sum(all_y_pred==y_ds)/len(y_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:16.250095Z",
          "start_time": "2021-06-15T22:52:16.221111Z"
        },
        "id": "GkEhwsuu0-r1"
      },
      "outputs": [],
      "source": [
        "def calc_f1_score(x_ds, y_ds, net):\n",
        "    all_y_pred,_ = test(x_ds,y_ds,net,ret=True)\n",
        "    f1_score = metrics.f1_score(y_true=y_ds,y_pred=all_y_pred)\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bfXYE6z50-r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369b90b1-842a-4424-f5a9-23a2325545f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score on the trg domain: 0.06224132081872667\n"
          ]
        }
      ],
      "source": [
        "only_src_f1_score = calc_f1_score(x_trg,y_trg, net)\n",
        "print('F1 score on the trg domain:',only_src_f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPD5p4GK0-r2"
      },
      "source": [
        "# Domain Adaptation with PPPL\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_pseudo_labels(trg_probs_np,trg_cp):\n",
        "    n_classes = 2\n",
        "    pseudo_labels = trg_probs_np.argmax(axis=1)\n",
        "    current_cp = np.zeros(n_classes)\n",
        "    for c in range(n_classes):\n",
        "        current_cp[c] = np.sum(pseudo_labels==c)/len(trg_probs_np)\n",
        "\n",
        "    diff_class_rates =  current_cp - trg_cp\n",
        "    for i in range(len(diff_class_rates)):\n",
        "        if diff_class_rates[i]<=0:\n",
        "            continue\n",
        "        predicted_as_c = pseudo_labels==i\n",
        "        current_class = i\n",
        "        current_diff = diff_class_rates[i]\n",
        "        current_num = np.round(current_diff*len(trg_probs_np)).astype(np.int32)\n",
        "\n",
        "        current_probs = trg_probs_np[pseudo_labels==current_class]\n",
        "        current_probs_sorted = np.sort(current_probs,axis=1)\n",
        "        current_certainty_scores = current_probs_sorted[:,-1] - current_probs_sorted[:,-2]\n",
        "        \n",
        "        current_certainty_scores_sorted_inds = np.argsort(current_certainty_scores)\n",
        "        y_val = np.ones(len(current_certainty_scores))*current_class\n",
        "        for j in range(current_num):\n",
        "            y_val[j]=1-current_class ###change pseudo-label to the opposite class!\n",
        "#             y_val[j]=-1\n",
        "        temp_pl = np.zeros(len(current_certainty_scores))\n",
        "        temp_pl[current_certainty_scores_sorted_inds] = y_val\n",
        "        pseudo_labels[predicted_as_c] = temp_pl\n",
        "    \n",
        "    return pseudo_labels"
      ],
      "metadata": {
        "id": "VyE2wO_tAlYQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step_DA_eager(x, y, w, optimizer,net):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = net(x, training=True)\n",
        "        mse_loss = tf.reduce_sum((y - y_pred)**2,axis=1)*w\n",
        "        mse_loss = tf.reduce_mean(mse_loss)\n",
        "    gradients = tape.gradient(mse_loss, net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
        "    train_loss(mse_loss)\n",
        "\n",
        "train_step_DA = tf.function(train_step_DA_eager)"
      ],
      "metadata": {
        "id": "bDiL2SrnApfn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_weights(nb_epochs,dhandler,tsolver):\n",
        "    total_batch = dhandler.len // dhandler.batch_size\n",
        "    if dhandler.len % dhandler.batch_size != 0:\n",
        "        total_batch += 1\n",
        "    st = time.time()\n",
        "    for ep in range(nb_epochs):\n",
        "        train_loss.reset_states()\n",
        "        for i in range(total_batch):\n",
        "            x_batch,y_batch_t,w_batch = dhandler.next_batch()\n",
        "            y_batch = np.zeros((dhandler.batch_size,2),dtype=np.float32)\n",
        "            y_batch[range(dhandler.batch_size),y_batch_t] = 1\n",
        "            train_step_DA(x_batch,y_batch,w_batch,tsolver.optimizer,tsolver.net)\n",
        "\n",
        "        passed_time = time.time() - st\n",
        "        remained_epochs = nb_epochs - ep\n",
        "        ETA = int(passed_time * remained_epochs)\n",
        "        ETA_min, ETA_sec = ETA // 60, ETA % 60\n",
        "        print ('\\r' + 'epoch: ' + str(ep + 1) + '/' + str(nb_epochs) + \\\n",
        "                      ' ETA: ' + str(ETA_min) + ':' + \"{0:02d}\".format(ETA_sec) + \\\n",
        "                      ' loss: ' + \"{0:0.4f}\".format(train_loss.result().numpy()),end=\" \")\n",
        "        sys.stdout.flush()\n",
        "    print(' ')"
      ],
      "metadata": {
        "id": "g86iGiLLA4t6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DA(tsolver,trg_data,trg_gts,src_data,src_gts,trg_cp,W = 0.):\n",
        "    begin_time = time.time()\n",
        "    my_coef = 0.05\n",
        "    weights_src = np.ones(len(src_gts))\n",
        "    trg_gts_unreal = np.zeros(len(trg_data))\n",
        "\n",
        "\n",
        "    for nnn in range(0,90,2):\n",
        "        print ('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i:',nnn//2 + 1,\n",
        "               'Elapsed Time(m): {0:0.2f}'.format((time.time()-begin_time)/60))\n",
        "        \n",
        "        #### Get scores and pseudo labels of the target domain\n",
        "        trg_pseudo_labels, trg_scores_np  = test(trg_data,trg_gts_unreal,tsolver.net,ret=True)\n",
        "\n",
        "        if nnn<60:\n",
        "            # Update target class proportions for scenario 3\n",
        "            for i in range(2):\n",
        "                temp = np.sum(trg_pseudo_labels==i)/len(trg_pseudo_labels)\n",
        "                trg_cp[i] = (1-W)*trg_cp[i] + W*temp\n",
        "\n",
        "        trg_pseudo_labels_adjusted = adjust_pseudo_labels(np.copy(trg_scores_np),trg_cp)\n",
        "        if nnn<60:\n",
        "            ### for the first 30 iterations instead of exclusion of samples \n",
        "            ### that their CP is larger than expected CP we change their pseudo-label\n",
        "            ### to the oposite class and keep them into the training set!\n",
        "            trg_pseudo_labels = trg_pseudo_labels_adjusted\n",
        "\n",
        "        ### Calculate the certainty scores for target samples\n",
        "        trg_scores_np_sorted = np.sort(trg_scores_np,axis=1)\n",
        "        certainty_scores = trg_scores_np_sorted[:,-1] - trg_scores_np_sorted[:,-2]\n",
        "        \n",
        "\n",
        "        ### Calculate weight for the target samples\n",
        "        weights_trg = np.zeros(len(certainty_scores))\n",
        "        for c in range(n_classes):\n",
        "            predicted_as_c = trg_pseudo_labels==c\n",
        "            size_c = np.sum(predicted_as_c)\n",
        "            if size_c>1:\n",
        "                left_size = int(np.ceil(((nnn+1)*0.01+0.1)*size_c))\n",
        "                x_val_left = 1+(10/2 - 1)/left_size*(np.arange(left_size))\n",
        "                right_size = size_c - left_size\n",
        "                x_val_right = 10000*(np.arange(1,right_size+1))\n",
        "                x_val = np.concatenate((x_val_left,x_val_right))\n",
        "                y_val = np.power(x_val,-1)\n",
        "                y_val = y_val[::-1]\n",
        "\n",
        "                cs_c = certainty_scores[predicted_as_c]\n",
        "                cs_c_sorted_inds = np.argsort(cs_c)\n",
        "                weights_trg2 = np.zeros(len(cs_c))\n",
        "                weights_trg2[cs_c_sorted_inds] = y_val\n",
        "                weights_trg[predicted_as_c] = weights_trg2\n",
        "                \n",
        "        ### Exclude\n",
        "        coef = (trg_pseudo_labels==trg_pseudo_labels_adjusted)*1\n",
        "        weights_trg*=coef\n",
        "        inclusion_condition = weights_trg>=0.001\n",
        "        trg_samples = trg_data[inclusion_condition]\n",
        "        trg_pseudo_labels = trg_pseudo_labels[inclusion_condition].astype(np.int32)\n",
        "        weights_trg = weights_trg[inclusion_condition]\n",
        "\n",
        "\n",
        "        #### Randomly select some samples from the source domain\n",
        "        p = np.random.permutation(len(src_data))\n",
        "        p = p[:len(trg_data)]\n",
        "        x_temp = src_data[p]\n",
        "        y_temp = src_gts[p]\n",
        "        w_temp = weights_src[:len(trg_data)]\n",
        "\n",
        "        #### Train Model\n",
        "        m1 = np.concatenate((x_temp,trg_samples))\n",
        "        m2 = np.concatenate((y_temp,trg_pseudo_labels)).astype(np.int32)\n",
        "        m3 = np.concatenate((w_temp,weights_trg)).astype(np.float32)\n",
        "        \n",
        "                                    \n",
        "        ### Balancing the positive and negative samples\n",
        "        pos_inds = m2==1\n",
        "        x_train_pos = m1[pos_inds]\n",
        "        y_train_pos = m2[pos_inds]\n",
        "        w_train_pos = m3[pos_inds]\n",
        "        x_train_neg = m1[~pos_inds]\n",
        "        y_train_neg = m2[~pos_inds]\n",
        "        w_train_neg = m3[~pos_inds]\n",
        "        p = np.random.permutation(len(x_train_neg))\n",
        "        x_train_neg = x_train_neg[p]\n",
        "        y_train_neg = y_train_neg[p]\n",
        "        w_train_neg = w_train_neg[p]\n",
        "        pos_len = len(x_train_pos)\n",
        "\n",
        "        DA_dhandler = DataHandler(np.concatenate((x_train_pos,x_train_neg[:pos_len]))\n",
        "                                ,np.concatenate((y_train_pos,y_train_neg[:pos_len]))\n",
        "                                ,np.concatenate((w_train_pos,w_train_neg[:pos_len])),batch_size=256,shuffle=True)            \n",
        "\n",
        "        ep = 1\n",
        "        train_model_with_weights(ep,DA_dhandler,tsolver)\n",
        "\n",
        "        if nnn%8==0:\n",
        "            current_f1 = calc_f1_score(trg_data,trg_gts, tsolver.net)\n",
        "            print('Current F1 score on the trg domain:',current_f1)"
      ],
      "metadata": {
        "id": "ccdkwY3uBPaq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_lr = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(lr=base_lr)\n",
        "grads = []\n",
        "for v in net.trainable_variables:\n",
        "    grads.append(np.zeros(v.shape,dtype=np.float32))\n",
        "optimizer.apply_gradients(zip(grads,net.trainable_variables))\n",
        "\n",
        "tsolver = Solver(optimizer,net,base_lr)"
      ],
      "metadata": {
        "id": "eUu4Z_tkImHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_gts = y_trg\n",
        "trg_data = x_trg\n",
        "src_gts = y_src\n",
        "src_data = x_src\n",
        "t_labels = np.array(trg_gts)\n",
        "s_labels = np.array(src_gts)\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "trg_gt_class_percentage = np.zeros(n_classes)\n",
        "for i in range(n_classes):\n",
        "    #Perfect knowledge about target domain class proportions\n",
        "    trg_gt_class_percentage[i] = np.sum(t_labels==i)/len(t_labels)\n"
      ],
      "metadata": {
        "id": "cxipw_JwIpOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DA(tsolver,trg_data,trg_gts,src_data,src_gts,trg_gt_class_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYNY6Gdu-mVR",
        "outputId": "e55be06e-8315-4edb-a87e-1cc0bf74b8ac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 1 Elapsed Time(m): 0.00\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.1453  \n",
            "Current F1 score on the trg domain: 0.3504262554737725\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 2 Elapsed Time(m): 0.14\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0250  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 3 Elapsed Time(m): 0.23\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0172  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 4 Elapsed Time(m): 0.32\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0203  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 5 Elapsed Time(m): 0.41\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0135  \n",
            "Current F1 score on the trg domain: 0.7745366234926306\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 6 Elapsed Time(m): 0.55\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0119  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 7 Elapsed Time(m): 0.65\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0113  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 8 Elapsed Time(m): 0.73\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0114  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 9 Elapsed Time(m): 0.82\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0103  \n",
            "Current F1 score on the trg domain: 0.850737316337366\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 10 Elapsed Time(m): 0.97\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0099  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 11 Elapsed Time(m): 1.07\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0097  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 12 Elapsed Time(m): 1.16\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0092  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 13 Elapsed Time(m): 1.26\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0086  \n",
            "Current F1 score on the trg domain: 0.9157241882524626\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 14 Elapsed Time(m): 1.41\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0086  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 15 Elapsed Time(m): 1.50\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0082  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 16 Elapsed Time(m): 1.60\n",
            "epoch: 1/1 ETA: 0:00 loss: 0.0083  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 17 Elapsed Time(m): 1.70\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0076  \n",
            "Current F1 score on the trg domain: 0.9229129602187564\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 18 Elapsed Time(m): 1.86\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0073  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 19 Elapsed Time(m): 1.96\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0070  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 20 Elapsed Time(m): 2.06\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0069  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 21 Elapsed Time(m): 2.16\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0066  \n",
            "Current F1 score on the trg domain: 0.9433581198935096\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 22 Elapsed Time(m): 2.32\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0063  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 23 Elapsed Time(m): 2.43\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0063  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 24 Elapsed Time(m): 2.54\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0061  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 25 Elapsed Time(m): 2.65\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0061  \n",
            "Current F1 score on the trg domain: 0.9548747526524298\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 26 Elapsed Time(m): 2.81\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0058  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 27 Elapsed Time(m): 2.92\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0056  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 28 Elapsed Time(m): 3.03\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0055  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 29 Elapsed Time(m): 3.14\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0054  \n",
            "Current F1 score on the trg domain: 0.9597904753303584\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 30 Elapsed Time(m): 3.32\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0054  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 31 Elapsed Time(m): 3.44\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0051  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 32 Elapsed Time(m): 3.55\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0050  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 33 Elapsed Time(m): 3.67\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0049  \n",
            "Current F1 score on the trg domain: 0.9595996180566987\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 34 Elapsed Time(m): 3.84\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0049  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 35 Elapsed Time(m): 3.96\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0047  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 36 Elapsed Time(m): 4.08\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0049  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 37 Elapsed Time(m): 4.20\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0047  \n",
            "Current F1 score on the trg domain: 0.9624082842891345\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 38 Elapsed Time(m): 4.38\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0046  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 39 Elapsed Time(m): 4.50\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0046  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 40 Elapsed Time(m): 4.63\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0046  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 41 Elapsed Time(m): 4.75\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0045  \n",
            "Current F1 score on the trg domain: 0.9664239399872192\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 42 Elapsed Time(m): 4.93\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0046  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 43 Elapsed Time(m): 5.06\n",
            "epoch: 1/1 ETA: 0:01 loss: 0.0047  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 44 Elapsed Time(m): 5.19\n",
            "epoch: 1/1 ETA: 0:02 loss: 0.0052  \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ i: 45 Elapsed Time(m): 5.31\n",
            "epoch: 1/1 ETA: 0:02 loss: 0.0058  \n",
            "Current F1 score on the trg domain: 0.9720219559185256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-15T22:52:17.283151Z",
          "start_time": "2021-06-15T22:52:17.259960Z"
        },
        "id": "Qh-U2Dlg0-r-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8318b8-ca5b-4368-fc69-9e41294bfe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current F1 score on the trg domain: 0.9720219559185256\n"
          ]
        }
      ],
      "source": [
        "current_f1 = calc_f1_score(trg_data,trg_gts,tsolver.net)\n",
        "print('Current F1 score on the trg domain:',current_f1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "412.186px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}